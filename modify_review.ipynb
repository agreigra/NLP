{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "# Fichier nlp.py a étudier!\n",
    "from nlp import text2tokens\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parallel apply on pandas dataframe\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "from nlp import text2tokens, text2vec\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from gensim.models import Word2Vec, KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'modified.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-dd729a0c8a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lecture de \"l'email\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"modified.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0memail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'modified.json'"
     ]
    }
   ],
   "source": [
    "# Lecture de \"l'email\"\n",
    "with open(\"modified.json\", \"r\") as fp:\n",
    "    email = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/cisd-\n",
      "[nltk_data]     hala/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARBAGE = {\"'s\", \"n't\", '...', 'oh',\"'m\", \"'re\", \"'\", \"''\", \"'ve\", \"'ll\", \"'d\", \"``\", \"-\"}\n",
    "STOP_WORDS = set(stopwords.words('english')).union(GARBAGE)\n",
    "\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [token.lower() for token in tokens if token not in string.punctuation]\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in STOP_WORDS]\n",
    "\n",
    "def sentence2tokens(sentence):\n",
    "    tokens = text2tokens(sentence)\n",
    "    tokens = clean_tokens(tokens)\n",
    "    tokens = remove_stop_words(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargons le modèle de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cisd-hala/.local/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "nlp_model = joblib.load('nlp_model.joblib')\n",
    "ml = nlp_model[\"ml\"]\n",
    "idf = nlp_model[\"idf\"]\n",
    "wv = nlp_model[\"wv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifions le calcul du sentiment identifié dans les textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment(text, wv, idf, ml, threshold=0.55):\n",
    "    # NLP : feature exctraction\n",
    "    tokens = text2tokens(text)\n",
    "    vector = text2vec(wv, idf, tokens)\n",
    "    # Compute prediction\n",
    "    prediction = ml.predict_proba(vector.reshape(1, -1))[0]\n",
    "    # Use positive class proba and threshold to estimate sentiment\n",
    "    sentiment = (prediction[1] > threshold)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargons le modèle de Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without giving too much away, there is a fade to white an hour into the film.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(email)):\n",
    "    appreciation = email[i][\"appreciation\"]\n",
    "    print(appreciation)\n",
    "    sentiment = compute_sentiment(appreciation, wv, idf, ml)\n",
    "    print(sentiment)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kind',\n",
       " 'item',\n",
       " 'find',\n",
       " 'packaged',\n",
       " '50',\n",
       " 'random',\n",
       " 'cheesefests',\n",
       " 'poverty',\n",
       " 'row',\n",
       " 'programmers']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appreciation = email[i][\"appreciation\"]\n",
    "sentiment = compute_sentiment(appreciation, wv, idf, ml)\n",
    "tokens = sentence2tokens(appreciation)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.44631943106651306),\n",
       " ('awful', 0.4452797770500183),\n",
       " ('good', 0.41658562421798706),\n",
       " ('horrible', 0.41330885887145996),\n",
       " ('dreadful', 0.3839857578277588),\n",
       " ('okay', 0.3715684413909912),\n",
       " ('poor', 0.35615599155426025),\n",
       " ('dumb', 0.35115286707878113),\n",
       " ('dolls', 0.34781792759895325),\n",
       " ('scary', 0.3412056863307953)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = wv.most_similar(positive = [\"bad\"], negative = [tokens[0]])\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_negative(appreciation):\n",
    "    i = 0\n",
    "    tokens = sentence2tokens(appreciation)\n",
    "    for i in range(len(tokens)):\n",
    "        if (tokens[i] in wv.vocab):\n",
    "            similar = wv.most_similar(positive = [\"bad\", \"negative\"], negative = [\"positive\", \"good\", tokens[i]])\n",
    "            appreciation = appreciation.replace(tokens[i], similar[0][0])\n",
    "            if(not compute_sentiment(appreciation, wv, idf, ml)):\n",
    "                break\n",
    "    return appreciation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_positive(appreciation):\n",
    "    i = 0\n",
    "    tokens = sentence2tokens(appreciation)\n",
    "    for i in range(len(tokens)):\n",
    "        if (tokens[i] in wv.vocab):\n",
    "            similar = wv.most_similar(positive = [\"good\", \"positive\"], negative = [\"negative\", \"bad\", tokens[i]] )\n",
    "            appreciation = appreciation.replace(tokens[i], similar[0][0])\n",
    "            if(compute_sentiment(appreciation, wv, idf, ml)):\n",
    "                break\n",
    "    return appreciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without heady too much away, there is a fade to white an hour into the film.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_appreciation = to_positive(appreciation)\n",
    "print(new_appreciation)\n",
    "sentiment = compute_sentiment(new_appreciation, wv, idf, ml)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = []\n",
    "expected_sentiments = 18 * [True] + 2 * [False]\n",
    "for i in range(len(email)):\n",
    "    student = email[i]\n",
    "    sentiment = compute_sentiment(student[\"appreciation\"], wv, idf, ml)\n",
    "    if (sentiment in expected_sentiments):\n",
    "        expected_sentiments.remove(sentiment)\n",
    "        found.append(i)\n",
    "for i in range(len(email)):\n",
    "    student = email[i]\n",
    "    appreciation = student[\"appreciation\"]\n",
    "    if (i in found):\n",
    "        continue\n",
    "    s = expected_sentiments.pop()\n",
    "    if (s):\n",
    "        new_appreciation = to_positive(appreciation)\n",
    "        email[i][\"appreciation\"] = new_appreciation\n",
    "    else:\n",
    "        new_appreciation = to_negative(aprreciation)\n",
    "        email[i][\"appreciation\"] = new_appreciation\n",
    "for i in range(len(email)):\n",
    "    student = email[i]\n",
    "    sentiment = compute_sentiment(student[\"appreciation\"], wv, idf, ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
